---
title: "MLProject"
author: "Raneem Hawari"
date: "2/1/2020"
output: html_document
---

## Procedure

We are using classe as the outcome variable. We are testing two models with Random Forest and Decision Tree. The most accurate model will be used as the final model.

## Cross Validation

We are using cross validation by subsampling the training set randomly sans TrainTrainingSet and Test Training Set data. The models will be fitted on the Train Training Set and the test will be conducted on the Test Training Set. I will choose the most accurate one and test the original Testing Set. 

## Expected out-of-sample error

The expected value in the out-of-sample error will be corresponding to  the expected # that are missclassified observations/total observations from the Test set. This is the quantity, one-accuracy that is located in the cross-validation set.
"Classe" is the factor variable. And it is splitting the Training set intor the TrainTrainingSet and TestTrainingSet. 

First load the required packages.
```{r}
library(lattice); library(ggplot2); library(caret); library(randomForest); library(rpart); library(rpart.plot);
```

```{r}
set.seed(1234)

# data load and clean up
trainingset <- read.csv("C:/Users/rhawa/Desktop/DSS/DSS_8_PracticalMachineLearning/Week4/pml-traininig.csv", na.strings=c("NA","#DIV/0!", ""))
testingset <- read.csv("C:/Users/rhawa/Desktop/DSS/DSS_8_PracticalMachineLearning/Week4/pml-testing.csv", na.strings=c("NA","#DIV/0!", ""))

# Perform exploratory analysis - 
# dim(trainingset); dim(testingset); summary(trainingset); summary(testingset); str(trainingset); str(testingset); head(trainingset); head(testingset);               

# Delete columns with all missing values
trainingset<-trainingset[,colSums(is.na(trainingset)) == 0]
testingset <-testingset[,colSums(is.na(testingset)) == 0]

# Delete variables are irrelevant to our current project: user_name, raw_timestamp_part_1, raw_timestamp_part_,2 cvtd_timestamp, new_window, and  num_window (columns 1 to 7). 
trainingset   <-trainingset[,-c(1:7)]
testingset <-testingset[,-c(1:7)]

# partition the data so that 75% of the training dataset into training and the remaining 25% to testing
traintrainset <- createDataPartition(y=trainingset$classe, p=0.75, list=FALSE)
TrainTrainingSet <- trainingset[traintrainset, ] 
TestTrainingSet <- trainingset[-traintrainset, ]

# The variable "classe" contains 5 levels: A, B, C, D and E. A plot of the outcome variable will allow us to see the frequency of each levels in the TrainTrainingSet data set and # compare one another.

plot(TrainTrainingSet$classe, col="blue", main="Plot of levels of variable classe within the TrainTrainingSet data set", xlab="classe", ylab="Frequency")
```

The graph above shows we can see that each level frequency is within the same order of magnitude of each other. Level A is the most frequent and level D is the least frequent.

## Prediction Model 1: Decision Tree

```{r}
model1 <- rpart(classe ~ ., data=TrainTrainingSet, method="class")

prediction1 <- predict(model1, TestTrainingSet, type = "class")

# Plot the Decision Tree
rpart.plot(model1, main="Classification Tree", extra=102, under=TRUE, faclen=0)
```

```{r}
# Test results on our TestTrainingSet data set:
confusionMatrix(prediction1, TestTrainingSet$classe)
```

## Prediction Model 2: Random Forest

```{r}
model2 <- randomForest(classe ~. , data=TrainTrainingSet, method="class")

# Predicting:
prediction2 <- predict(model2, TestTrainingSet, type = "class")

# Test results on TestTrainingSet data set:
confusionMatrix(prediction2, TestTrainingSet$classe)
```

## Prediction Model Final Decision

We observe that Random Forest is better than the Decision Treees. Random Forest was 0.995 (95% CI: (0993, 0.997)) compared to Decision Tree 0.739 (95% CI: (0.727, 0.752)). We choose the Random Forests model. the out-of-sample error is estimated at 0.005, or 0.5%.

## Tests

This is the Final Test prediction outcome from the Random Forest with the testing dataset.

```{r}
# predict outcome levels on the original Testing data set using Random Forest algorithm
predictfinal <- predict(model2, testingset, type="class")
predictfinal
```

